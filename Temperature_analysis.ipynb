{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Impact of Ambient Temperature on Server Efficiency\n",
    "\n",
    "\n",
    "Hypothesis: Server power consumption increases as temperature increases reducing server efficiency. As PUE values approach 1 an increasing portion of the Data centre's power is used in the server therfore there is likely to be a trade-off on operating temperature depending on cooling infrastructure and number of servers in the datacenter. \n",
    "\n",
    "------\n",
    "\n",
    "Plan:\n",
    "\n",
    "- Load in all of the SERT results avoiding any invalid ones\n",
    "- Merge data as needed \n",
    "- Generate graphs showing power consumption against load and temperature\n",
    "- Find a trade-off between operating temperature and number of servers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from parse_results import process_results_xml\n",
    "import yaml\n",
    "#import influxdb_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "settings_file = Path('settings.yaml')\n",
    "\n",
    "if settings_file.exists():\n",
    "    with settings_file.open() as f:\n",
    "        params = yaml.load(f, Loader=yaml.FullLoader)\n",
    "else:\n",
    "    params = {}\n",
    "    \n",
    "sert_results_dir = params.get('results_dir', 'sert_results')\n",
    "bios_setting_file = params.get('test_settings', 'test_settings.csv')\n",
    "cpu_metrics_dir = params.get('cpu_metrics_dir', 'cpu_data')\n",
    "\n",
    "working_dir_path = params.get('temp_dir', 'temp_dir')\n",
    "all_data_file = params.get('data_file', 'all_data.csv')\n",
    "overwrite_data = params.get('overwrite_data', False)\n",
    "\n",
    "whitelist = params.get('whitelist', '')\n",
    "\n",
    "working_dir = Path(working_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and load the data (Run Once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_sert(file, test_name, test_details, params):\n",
    "    # Load the SERT result\n",
    "    metrics, score, env = process_results_xml(f)\n",
    "    \n",
    "    # METRICS\n",
    "    metrics_df = pd.DataFrame.from_records(metrics)\n",
    "    # Remove calibration runs but record the calibration score against each loadlevel to calculate actual loadlevel\n",
    "    calibrations = metrics_df.loc[metrics_df['loadlevel']=='calibration', ['worklet', 'score']]\n",
    "    calibrations = calibrations.rename(columns={'score': 'calibration-score'})\n",
    "\n",
    "    metrics_df = pd.merge(metrics_df.drop(index=calibrations.index), calibrations, how='left', on='worklet')\n",
    "    metrics_df['actual-load'] = metrics_df['score'] / metrics_df['calibration-score']\n",
    "    metrics_df.loc[metrics_df['workload'] == 'Idle', 'actual-load'] = 0\n",
    "    metrics_df['test-name'] = test_name\n",
    "    \n",
    "    if test_details['location'].get(test_name) == 'Tunnel':\n",
    "        if 'influxdb' in params:\n",
    "            metrics_df['pressure'] = get_tunnel_pressure(metrics_df, params)\n",
    "        else:\n",
    "            # Can't get tunnel data without influx credentials\n",
    "            metrics_df['pressure'] = np.nan\n",
    "    else:\n",
    "        metrics_df['pressure'] = 0.0\n",
    "\n",
    "    # SCORE\n",
    "    score_df = pd.DataFrame.from_records(score)\n",
    "    score_df['test-name'] = test_name\n",
    "    \n",
    "    # ENV\n",
    "    env_df = pd.DataFrame.from_records(env, index=[test_name])\n",
    "    # Test details are \"unknwon\" if not in the test_details csv\n",
    "    env_df['location'] = test_details['location'].get(test_name, 'unknown')\n",
    "    env_df['bios'] = test_details['bios'].get(test_name, 'unknown')\n",
    "    \n",
    "    return metrics_df, env_df, score_df\n",
    "    \n",
    "def get_tunnel_pressure(metrics, params):\n",
    "    from influxdb import InfluxDBClient\n",
    "    client = InfluxDBClient(host=params['influxdb']['host'], \n",
    "                            port=params['influxdb']['port'], \n",
    "                            username=params['influxdb']['user'], \n",
    "                            password=params['influxdb']['password'],\n",
    "                            database=params['influxdb']['tunnel-database'])\n",
    "    \n",
    "    return metrics.apply(get_pressure_row,  axis=1, client=client)\n",
    "    \n",
    "def get_pressure_row(r, client):\n",
    "    query = f'select (mean(\"value\")-21.65)*62/19 from sensors where \"channel\"=\\'Pressure\\' and time>=\\'{r.start.isoformat()}\\' and time<=\\'{r.end.isoformat()}\\''\n",
    "    result = client.query(query)\n",
    "    for pt in result.get_points('sensors'):\n",
    "        return pt['mean']\n",
    "\n",
    "    return np.nan # Tunnel run but no data    \n",
    "    \n",
    "\n",
    "# Ensure the working directory exists\n",
    "if not working_dir.exists():\n",
    "    working_dir.mkdir(parents=True)\n",
    "\n",
    "# Temp files\n",
    "metrics_path = working_dir.joinpath('metrics.csv')\n",
    "scores_path = working_dir.joinpath('scores.csv')\n",
    "details_path = working_dir.joinpath('test_details.csv')\n",
    "\n",
    "    \n",
    "# Load any existing chached data or start empty if they don't exist\n",
    "if working_dir.joinpath('metrics.csv').exists() and not overwrite_data:\n",
    "    print('Loading SERT data from disk')\n",
    "    try:\n",
    "        metrics_data = pd.read_csv(str(metrics_path), parse_dates=['start', 'end'])\n",
    "        scores = pd.read_csv(str(scores_path))\n",
    "        test_details = pd.read_csv(str(details_path), index_col=0)\n",
    "    except Exception as e:\n",
    "        print('Reloading failed, exception: ', e, '\\nRebuilding...')\n",
    "        metrics_data = pd.DataFrame()\n",
    "        test_details = pd.DataFrame()\n",
    "        scores = pd.DataFrame()\n",
    "else:\n",
    "    print('Rebuilding SERT results data.....')\n",
    "    metrics_data = pd.DataFrame()\n",
    "    test_details = pd.DataFrame()\n",
    "    scores = pd.DataFrame()\n",
    "\n",
    "# Load external details for test\n",
    "if bios_setting_file != '' and Path(bios_setting_file).is_file():\n",
    "    settings = pd.read_csv(bios_setting_file, index_col=0)\n",
    "else:\n",
    "    settings = pd.DataFrame(columns=['location', 'bios'])\n",
    "    \n",
    "# Find any results in the results directory that aren't already in the dataframes, but only look for results that are valid or in the whitelist\n",
    "source_path = Path(sert_results_dir)\n",
    "new_results = [x for x in source_path.glob('**//results.xml') if x.parent.name not in test_details.index and \n",
    "                                                               (not x.parent.joinpath('invalid.png').exists() or x.parent.name in whitelist)]\n",
    "\n",
    "for f in new_results:     \n",
    "    try:\n",
    "        metrics, details, score = aggregate_sert(str(f), f.parent.name, settings, params)\n",
    "\n",
    "        metrics_data = metrics_data.append(metrics, ignore_index=True)\n",
    "        test_details = test_details.append(details)\n",
    "        scores = scores.append(score, ignore_index=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f, ': FAILED TO LOAD -- ', e, type(e))\n",
    "\n",
    "        \n",
    "# Store the new metrics\n",
    "if len(new_results) > 0:\n",
    "    # Store generated data\n",
    "    metrics_data.to_csv(str(metrics_path), index=False)\n",
    "    test_details.to_csv(str(details_path))\n",
    "    scores.to_csv(str(scores_path), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to True if you haven't run the previous cell and want to just load the data from last time.\n",
    "reload_from_cached = False\n",
    "\n",
    "data_cache_path = working_dir.joinpath(all_data_file)\n",
    "\n",
    "if reload_from_cached and data_cache_path.exists():\n",
    "    sert_data = pd.read_csv(str(data_cache_path), parse_dates=['start', 'end'])\n",
    "else:        \n",
    "    # Build the combined view for analysis\n",
    "\n",
    "    # Join metrics, test details and scores into a big view table\n",
    "    sert_data = pd.merge(metrics_data, scores[['test-name', 'worklet', 'loadlevel', 'norm-score', 'ref-score', 'efficiency-score']], how='left', on=['test-name', 'worklet', 'loadlevel'])\n",
    "    sert_data = pd.merge(sert_data, test_details, left_on='test-name', right_index=True)\n",
    "\n",
    "    # Scenario column for easier display and filtering\n",
    "    sert_data['scenario'] = list(' - '.join(s) for s in zip(sert_data['model'], sert_data['cpu']))\n",
    "\n",
    "    # Drop any tests that were run without hyperthreading\n",
    "    sert_data.drop(sert_data[sert_data.logical_cores == sert_data.physical_cores].index, inplace=True)\n",
    "    \n",
    "    # Save a cache of the joins\n",
    "    sert_data.to_csv(str(data_cache_path), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sert_data.head()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sert_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_details\n",
    "total_score = scores[scores['workload'] == 'All'].dropna(axis=1)\n",
    "total_score = total_score.merge(test_details, left_on='test-name', right_index=True, how='left').dropna()\n",
    "total_score['scenario'] = list(' - '.join(x) for x in zip(total_score['model'], total_score['cpu']))\n",
    "total_score['temperature'] = total_score.apply(lambda row: sert_data[sert_data['test-name'] == row['test-name']]['temp-avg'].mean(), axis=1)\n",
    "total_score['temperature-range'] = total_score.apply(lambda row: sert_data[sert_data['test-name'] == row['test-name']]['temp-max'].max() - sert_data[sert_data['test-name'] == row['test-name']]['temp-min'].min(), axis=1)\n",
    "total_score = total_score.merge(sert_data[sert_data['workload'] == 'Idle'][['watts-avg','test-name']], left_on='test-name', right_on='test-name').rename(columns={'watts-avg':'idle-power'})\n",
    "total_score['max-power'] = total_score.apply(lambda row: sert_data[(sert_data['test-name'] == row['test-name']) & (sert_data['loadlevel']=='100%')]['watts-avg'].mean(), axis=1)\n",
    "total_score['pressure'] = total_score.apply(lambda row: sert_data[sert_data['test-name'] == row['test-name']]['pressure'].mean(), axis=1)\n",
    "\n",
    "total_score['pressure-bound'] = pd.cut(total_score['pressure'],[total_score.pressure.min(), 15, total_score.pressure.max()],labels=['low','high'])\n",
    "total_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What scenarios have been tested?\n",
    "Using a 3 bin strategy for temperature testing and high and low for pressure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tested = total_score.groupby(['scenario', 'bios', total_score['pressure-bound'], pd.cut(total_score.temperature,[20, 23.33,26.66, 30])]).size().unstack().unstack()\n",
    "tested.style.applymap(lambda x: 'background-color:pink' if x < 3 else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_score[(total_score['bios'] == 'unknown')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sert_data = sert_data[(sert_data['bios'] == 'Performance')| (sert_data['bios'] == 'Efficiency')]\n",
    "\n",
    "total_score = total_score[(total_score['bios'] == 'Performance')| (total_score['bios'] == 'Efficiency')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sert_data['pressure-bound'] = pd.cut(sert_data['pressure'],[sert_data.pressure.min(), 15, sert_data.pressure.max()],labels=['low','high'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean\\n', sert_data.groupby('scenario')['temp-avg'].mean())\n",
    "print('\\nVariance\\n', sert_data.groupby('scenario')['temp-avg'].var())\n",
    "sns.displot(data=sert_data, x=\"temp-avg\", hue=\"scenario\", kde=True, fill=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=sert_data, x=\"pressure\", hue=\"scenario\", kde=True, fill=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(sert_data[sert_data.logical_cores == sert_data.physical_cores]['test-name'].unique()), list(sert_data[sert_data.logical_cores == sert_data.physical_cores]['bios'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effects on overall SERT score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficiency and power consumption measured by SERT\n",
    "\n",
    "For the CPU workelts in particular, we can plot the benchmark load against the efficiency score achevied for each scenario. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu = sert_data[(sert_data['workload'] == 'CPU')| (sert_data['workload'] == 'Idle')]\n",
    "\n",
    "sns.lmplot(x='actual-load', y='efficiency-score', hue='scenario', col='bios', \n",
    "           data=cpu[cpu['temp-avg'] < 23.66], order=2, truncate=True, scatter=True).fig.suptitle('CPU Worklet Efficiency Scores', y=1.1)\n",
    "sns.lmplot(x='actual-load', y='watts-avg', hue='scenario', col='bios', \n",
    "           data=cpu[cpu['temp-avg'] < 23.66], order=2, truncate=True, scatter=True).fig.suptitle('CPU Worklet Power Consumption', y=1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cleaner plot without the individual data plotted for each sert run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='actual-load', y='efficiency-score', hue='scenario', data=cpu[cpu['temp-avg'] < 23.66], order=2, truncate=True, scatter=False)\n",
    "ax = plt.gca()\n",
    "ax.set_title('CPU Worklet Efficiency Scores ( Test Temperature < 30C)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environmental conditions for the tests are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Effect of Temperature\n",
    "The overall efficiency score across various temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=total_score, x='temperature', y='efficiency-score', hue='bios', row='scenario', col='pressure-bound')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=total_score, x='temperature', y='max-power', hue='pressure-bound', row='scenario', col='bios')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_score[total_score['temperature'] < 22].groupby(['scenario', 'bios'])[['efficiency-score', 'temperature', 'idle-power', 'max-power']].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_score[total_score['temperature'] > 28].groupby(['scenario', 'bios'])[['efficiency-score', 'temperature', 'idle-power', 'max-power']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu[(cpu['loadlevel'] == '100%')].groupby([ 'worklet','scenario', 'bios'])['norm-score'].mean().unstack().pct_change(axis=1).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu[(cpu.model == 'PowerEdge R640') & (cpu.worklet == 'CryptoAES') & (cpu.loadlevel == '100%')].groupby('bios')['norm-score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu[['worklet', 'loadlevel', 'scenario', 'bios', 'score']].groupby(['scenario', 'bios', 'worklet', 'loadlevel']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPU Power\n",
    "\n",
    "The CPU is usually considered the driver of most power consumption in the server (excluding any expansion cards). During the SERT tests we have also recorded low-level performance registers of the CPU like per-core frequency and also power consumption. \n",
    "\n",
    "Todo: \n",
    "- Determine relationship between chassis and CPU power consumption\n",
    "    - Assume power = P_Idle + P_Chassis + P_CPU\n",
    "    - IS P_Chassis a function of CPU power?\n",
    "    \n",
    "    \n",
    "Read the CPU power data in and summarise for the tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_metrics = pd.DataFrame()\n",
    "\n",
    "if not os.path.exists(f'{working_dir}//cpu_metrics.csv') or overwrite_data :\n",
    "    \n",
    "    for f in glob.glob(f'{cpu_metrics_dir}//**.csv', recursive=True):\n",
    "        try:\n",
    "            samples = pd.read_csv(f, skiprows=8, header=0, index_col=0, parse_dates=['Time'], encoding='cp1252')\n",
    "            if not pd.api.types.is_datetime64_any_dtype(samples.index.dtype):\n",
    "                # Final row contains \"Session end:\"\n",
    "                samples.drop('Session end:', inplace=True)\n",
    "                samples.index = pd.to_datetime(samples.index)\n",
    "\n",
    "            cpu_metrics = cpu_metrics.append(samples)\n",
    "        except:\n",
    "            print(f'FAILED LOADING FILE: {f}')\n",
    "\n",
    "    cpu_metrics.sort_index(inplace=True)\n",
    "    cpu_metrics['total cpu power'] = cpu_metrics['CPU 0 Power'] + cpu_metrics['CPU 1 Power']\n",
    "    \n",
    "    cpu_metrics.to_csv(f'{working_dir}//cpu_metrics.csv')\n",
    "    \n",
    "else:\n",
    "    cpu_metrics = pd.read_csv(f'{working_dir}//cpu_metrics.csv', index_col='Time', parse_dates=['Time'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_metrics['avg-temp'] = cpu_metrics.filter(regex='Temp').mean(axis=1, skipna=True)\n",
    "cpu_metrics['avg-load'] = cpu_metrics.filter(regex='load').mean(axis=1, skipna=True)\n",
    "cpu_metrics['avg-freq'] = cpu_metrics.filter(regex='speed').mean(axis=1, skipna=True)\n",
    "\n",
    "\n",
    "#pd.to_datetime(cpu_metrics['Time']\n",
    "sert_data['cpu-power'] = sert_data.apply(lambda row: cpu_metrics['total cpu power'][row['start'].tz_localize(None):row['end'].tz_localize(None)].mean(), axis=1)\n",
    "sert_data['chassis-power'] = sert_data['watts-avg'] - sert_data['cpu-power']\n",
    "\n",
    "sert_data['cpu-temp'] = sert_data.apply(lambda row: cpu_metrics['avg-temp'][row['start'].tz_localize(None):row['end'].tz_localize(None)].mean(), axis=1)\n",
    "sert_data['cpu-load'] = sert_data.apply(lambda row: cpu_metrics['avg-load'][row['start'].tz_localize(None):row['end'].tz_localize(None)].mean(), axis=1)\n",
    "sert_data['cpu-freq'] = sert_data.apply(lambda row: cpu_metrics['avg-freq'][row['start'].tz_localize(None):row['end'].tz_localize(None)].mean(), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking again at the server efficiency scores, but now using the CPU utilsiation dat from the OS rather than the load data calculated by SERT. SERT load is a proportion of the total score/transactions acheived during the calibration runs. OS CPU utilisaiton is the proportion of time the CPU is busy performing operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_worklets = sert_data[(sert_data['workload'] == 'Idle') | (sert_data['workload'] == 'CPU')]\n",
    "cpu_worklets['scenario'] = list(' - '.join(x) for x in zip(cpu_worklets['model'], cpu_worklets['cpu']))\n",
    "sns.lmplot(x='cpu-load', y='efficiency-score', hue='scenario', col='pressure-bound', row='bios', data=cpu_worklets[cpu_worklets['temp-avg'] < 30], order=2, truncate=True, scatter=True).fig.suptitle('Efficiency Scores ( Test Temperature < 22.5C)', y=1.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a significantly different relationship than that shown for the SERT load. \n",
    "\n",
    "Breaking down the performance per server and per worklet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    sns.lmplot(x='actual-load', y='efficiency-score', hue='worklet', col='bios', row='scenario', data=cpu_worklets[cpu_worklets['scenario']=='PowerEdge R640 - Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz'], order=2, truncate=True, scatter=True).fig.suptitle(f'Efficiency Scores', y=1.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    sns.lmplot(x='actual-load', y='efficiency-score', hue='worklet', col='bios', row='scenario', data=cpu_worklets[cpu_worklets['scenario']=='PowerEdge R620 - Intel(R) Xeon(R) CPU E5-2690 0 @ 2.90GHz'], order=2, truncate=True, scatter=True).fig.suptitle(f'Efficiency Scores', y=1.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    sns.lmplot(x='actual-load', y='efficiency-score', hue='worklet', col='bios', row='scenario', data=cpu_worklets[cpu_worklets['scenario']=='PowerEdge R640 - Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz'], order=2, truncate=True, scatter=True).fig.suptitle(f'Efficiency Scores', y=1.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='temp-avg', y='efficiency-score', hue='scenario', data=cpu_worklets, order=2, scatter=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPU power consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdf = cpu_worklets.melt('actual-load', ['watts-avg', 'cpu-power', 'chassis-power'])\n",
    "\n",
    "sns.lmplot(x='actual-load', y='value', hue='variable', data=plotdf, order=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for server in cpu_worklets['scenario'].unique():\n",
    "    plotdf = cpu_worklets[cpu_worklets['scenario'] == server].melt(['actual-load', 'bios'], ['watts-avg', 'cpu-power', 'chassis-power'])\n",
    "\n",
    "    sns.lmplot(x='actual-load', y='value', col='bios', hue='variable', data=plotdf, order=2).fig.suptitle(f'Power Breakdown - {server}', y=1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for server in cpu_worklets['scenario'].unique():\n",
    "    plotdf = cpu_worklets[cpu_worklets['scenario'] == server].melt(['cpu-load', 'bios'], ['watts-avg', 'cpu-power', 'chassis-power'])\n",
    "\n",
    "    sns.lmplot(x='actual-load', y='value', col='bios', hue='variable', data=plotdf, order=2).fig.suptitle(f'Power Breakdown - {server}', y=1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for worklet in sert_data[sert_data['workload'] == 'CPU']['worklet'].unique():\n",
    "    sns.lmplot(data=sert_data[(sert_data['worklet'] == worklet) | (sert_data['workload'] == 'Idle')], x='actual-load', y='cpu-load', hue='scenario', col='bios', order=2).fig.suptitle(f'Server load vs CPU utilisaiton - {worklet}', y=1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu = sert_data[(sert_data['workload'] == 'CPU')| (sert_data['workload'] == 'Idle')]\n",
    "cpu = cpu[cpu['model'] == 'PowerEdge R620']\n",
    "#cpu = cpu[cpu['cpu'].str.contains('E5-2690 0')]\n",
    "\n",
    "\n",
    "#sns.lmplot(x='cpu-power', y='chassis-power', hue='worklet', data=cpu, order=2)\n",
    "sns.scatterplot(x='temp-avg', y='watts-avg', hue='cpu', data=cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='cpu-load', y='cpu-power', data=cpu, order=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.scatter(x=cpu['cpu-temp'], y=cpu['cpu-power'], c=cpu['temp-avg'])\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_ylabel('Ambient Temp', rotation=90)\n",
    "plt.ylabel('CPU Power')\n",
    "plt.xlabel('CPU Temp')\n",
    "plt.title('R620 E5-2690, Efficiency (DPAC) Mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.scatter(x=cpu['cpu-temp'], y=cpu['cpu-power'], c=cpu['cpu-load'])\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_ylabel('CPU Load', rotation=90)\n",
    "plt.ylabel('CPU Power')\n",
    "plt.xlabel('CPU Temp')\n",
    "plt.title('R620 E5-2690, Efficiency (DPAC) Mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.scatter(x=cpu['cpu-freq'], y=cpu['cpu-power'], c=cpu['cpu-load'])\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_ylabel('CPU Load', rotation=90)\n",
    "plt.ylabel('CPU Power')\n",
    "plt.xlabel('CPU Freq')\n",
    "plt.title('R620 E5-2690, Efficiency (DPAC) Mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "9fc60a84865c999635346deed1634b827bc0da000474eb18a195b0c8155b9d86"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('data-analysis': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
