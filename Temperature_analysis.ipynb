{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Impact of Ambient Temperature on Server Efficiency\n",
    "\n",
    "\n",
    "Hypothesis: Server power consumption increases as temperature increases reducing server efficiency. As PUE values approach 1 an increasing portion of the Data centre's power is used in the server therfore there is likely to be a trade-off on operating temperature depending on cooling infrastructure and number of servers in the datacenter. \n",
    "\n",
    "------\n",
    "\n",
    "Plan:\n",
    "\n",
    "- Load in all of the SERT results avoiding any invalid ones\n",
    "- Merge data as needed \n",
    "- Generate graphs showing power consumption against load and temperature\n",
    "- Find a trade-off between operating temperature and number of servers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from parse_results import process_results_xml\n",
    "import yaml\n",
    "#import influxdb_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "settings_file = Path('settings.yaml')\n",
    "\n",
    "if settings_file.exists():\n",
    "    with settings_file.open() as f:\n",
    "        params = yaml.load(f, Loader=yaml.FullLoader)\n",
    "else:\n",
    "    params = {}\n",
    "    \n",
    "sert_results_dir = params.get('results_dir', 'sert_results')\n",
    "bios_setting_file = params.get('test_settings', 'test_settings.csv')\n",
    "cpu_metrics_dir = params.get('cpu_metrics_dir', 'cpu_data')\n",
    "\n",
    "working_dir_path = params.get('temp_dir', 'temp_dir')\n",
    "all_data_file = params.get('data_file', 'all_data.csv')\n",
    "overwrite_data = params.get('overwrite_data', False)\n",
    "\n",
    "working_dir = Path(working_dir_path)\n",
    "#overwrite_data=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and load the data (Run Once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_sert(file, test_name, test_details, params):\n",
    "    # Load the SERT result\n",
    "    metrics, score, env = process_results_xml(f)\n",
    "    \n",
    "    # METRICS\n",
    "    metrics_df = pd.DataFrame.from_records(metrics)\n",
    "    # Remove calibration runs but record the calibration score against each loadlevel to calculate actual loadlevel\n",
    "    calibrations = metrics_df.loc[metrics_df['loadlevel']=='calibration', ['worklet', 'score']]\n",
    "    calibrations = calibrations.rename(columns={'score': 'calibration-score'})\n",
    "\n",
    "    metrics_df = pd.merge(metrics_df.drop(index=calibrations.index), calibrations, how='left', on='worklet')\n",
    "    metrics_df['actual-load'] = metrics_df['score'] / metrics_df['calibration-score']\n",
    "    metrics_df.loc[metrics_df['workload'] == 'Idle', 'actual-load'] = 0\n",
    "    metrics_df['test-name'] = test_name\n",
    "    \n",
    "    if test_details['location'].get(test_name) == 'Tunnel':\n",
    "        if 'influxdb' in params:\n",
    "            metrics_df['pressure'] = get_tunnel_pressure(metrics_df, params)\n",
    "        else:\n",
    "            # Can't get tunnel data without influx credentials\n",
    "            metrics_df['pressure'] = np.nan\n",
    "    else:\n",
    "        metrics_df['pressure'] = 0.0\n",
    "\n",
    "    # SCORE\n",
    "    score_df = pd.DataFrame.from_records(score)\n",
    "    score_df['test-name'] = test_name\n",
    "    \n",
    "    # ENV\n",
    "    env_df = pd.DataFrame.from_records(env, index=[test_name])\n",
    "    # Test details are \"unknwon\" if not in the test_details csv\n",
    "    env_df['location'] = test_details['location'].get(test_name, 'unknown')\n",
    "    env_df['bios'] = test_details['bios'].get(test_name, 'unknown')\n",
    "    \n",
    "    return metrics_df, env_df, score_df\n",
    "    \n",
    "def get_tunnel_pressure(metrics, params):\n",
    "    from influxdb import InfluxDBClient\n",
    "    client = InfluxDBClient(host=params['influxdb']['host'], \n",
    "                            port=params['influxdb']['port'], \n",
    "                            username=params['influxdb']['user'], \n",
    "                            password=params['influxdb']['password'],\n",
    "                            database=params['influxdb']['tunnel-database'])\n",
    "    \n",
    "    return metrics.apply(get_pressure_row,  axis=1, client=client)\n",
    "    \n",
    "def get_pressure_row(r, client):\n",
    "    query = f'select (mean(\"value\")-21.65)*62/19 from sensors where \"channel\"=\\'Pressure\\' and time>=\\'{r.start.isoformat()}\\' and time<=\\'{r.end.isoformat()}\\''\n",
    "    result = client.query(query)\n",
    "    for pt in result.get_points('sensors'):\n",
    "        return pt['mean']\n",
    "\n",
    "    return np.nan # Tunnel run but no data    \n",
    "    \n",
    "\n",
    "# Ensure the working directory exists\n",
    "if not working_dir.exists():\n",
    "    working_dir.mkdir(parents=True)\n",
    "\n",
    "# Temp files\n",
    "metrics_path = working_dir.joinpath('metrics.csv')\n",
    "scores_path = working_dir.joinpath('scores.csv')\n",
    "details_path = working_dir.joinpath('test_details.csv')\n",
    "\n",
    "    \n",
    "# Load any existing chached data or start empty if they don't exist\n",
    "if metrics_path.exists() and not overwrite_data:\n",
    "    print('Loading SERT data from disk')\n",
    "    try:\n",
    "        metrics_data = pd.read_csv(str(metrics_path), parse_dates=['start', 'end'])\n",
    "        scores = pd.read_csv(str(scores_path))\n",
    "        test_details = pd.read_csv(str(details_path), index_col=0)\n",
    "    except Exception as e:\n",
    "        print('Reloading failed, exception: ', e, '\\nRebuilding...')\n",
    "        metrics_data = pd.DataFrame()\n",
    "        test_details = pd.DataFrame()\n",
    "        scores = pd.DataFrame()\n",
    "else:\n",
    "    print('Rebuilding SERT results data.....')\n",
    "    metrics_data = pd.DataFrame()\n",
    "    test_details = pd.DataFrame()\n",
    "    scores = pd.DataFrame()\n",
    "\n",
    "# Load external details for test\n",
    "if bios_setting_file != '' and Path(bios_setting_file).is_file():\n",
    "    settings = pd.read_csv(bios_setting_file, index_col=0)\n",
    "else:\n",
    "    raise Exception(f'Test scenario settings file not found. Cannot continue.')\n",
    "\n",
    "#  Fill in invalid tests\n",
    "if 'invalid' not in test_details.columns and len(test_details.index) > 0:\n",
    "    invalid_tests = [f.parent.name for f in source_path.glob('**/invalid.png')]\n",
    "    test_details['invalid'] = test_details.index.isin(invalid_tests)\n",
    "\n",
    "    \n",
    "# Find any results in the results directory that aren't already in the dataframes, but only look for results that are valid or in the whitelist\n",
    "source_path = Path(sert_results_dir)\n",
    "new_results = [source_path.joinpath(test).joinpath('results.xml') for test in settings.index if test not in test_details.index]\n",
    "\n",
    "failed_files = []\n",
    "\n",
    "for f in new_results:     \n",
    "    try:\n",
    "        metrics, details, score = aggregate_sert(str(f), f.parent.name, settings, params)\n",
    "\n",
    "        # Indicate whether the test was marked invalid by SERT\n",
    "        details['invalid'] = f.parent.joinpath('invalid.png').exists()\n",
    "\n",
    "        metrics_data = metrics_data.append(metrics, ignore_index=True)\n",
    "        test_details = test_details.append(details)\n",
    "        scores = scores.append(score, ignore_index=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f.parent.name, ' FAILED TO LOAD -- ', e, type(e))\n",
    "        failed_files += f.name\n",
    "\n",
    "\n",
    "# Store the new metrics\n",
    "if len(new_results) > 0:\n",
    "    # Store generated data\n",
    "    metrics_data.to_csv(str(metrics_path), index=False)\n",
    "    test_details.to_csv(str(details_path))\n",
    "    scores.to_csv(str(scores_path), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to True if you haven't run the previous cell and want to just load the data from last time.\n",
    "reload_from_cached = True\n",
    "\n",
    "data_cache_path = working_dir.joinpath(all_data_file)\n",
    "\n",
    "if reload_from_cached and data_cache_path.exists():\n",
    "    sert_data = pd.read_csv(str(data_cache_path), parse_dates=['start', 'end'])\n",
    "else:\n",
    "    sert_data = pd.DataFrame(columns=['test-name'])\n",
    "\n",
    "new_tests = test_details.index[~test_details.index.isin(sert_data['test-name'].unique())]\n",
    "\n",
    "if len(new_tests) > 0:\n",
    "    # Build the combined view for analysis\n",
    "\n",
    "    # Join metrics, test details and scores into a big view table\n",
    "    new_data = pd.merge(metrics_data[metrics_data['test-name'].isin(new_tests)], scores[['test-name', 'worklet', 'loadlevel', 'norm-score', 'ref-score', 'efficiency-score']], how='left', on=['test-name', 'worklet', 'loadlevel'])\n",
    "    new_data = pd.merge(new_data, test_details, left_on='test-name', right_index=True)\n",
    "\n",
    "    # Scenario column for easier display and filtering\n",
    "    new_data['scenario'] = list(' - '.join(s) for s in zip(new_data['model'], new_data['cpu']))\n",
    "\n",
    "    # Drop any tests that were run without hyperthreading\n",
    "    new_data.drop(new_data[new_data.logical_cores == new_data.physical_cores].index, inplace=True)\n",
    "    \n",
    "\n",
    "    sert_data = sert_data.append(new_data)\n",
    "    # Save a cache of the joins\n",
    "    sert_data.to_csv(str(data_cache_path), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any tests need to be removed\n",
    "recently_excluded_tests = test_details.index[~test_details.index.isin(settings.index)]\n",
    "\n",
    "if len(recently_excluded_tests) > 0:\n",
    "    print(f'Removing {len(recently_excluded_tests)} tests that have been removed from the test settings file')\n",
    "    test_details.drop(recently_excluded_tests, inplace=True)\n",
    "    metrics_data.drop(metrics_data.index[metrics_data['test-name'].isin(recently_excluded_tests)], inplace=True)\n",
    "    scores.drop(scores.index[scores['test-name'].isin(recently_excluded_tests)], inplace=True)\n",
    "    sert_data.drop(sert_data.index[sert_data['test-name'].isin(recently_excluded_tests)], inplace=True)\n",
    "\n",
    "    # Save Changes\n",
    "    metrics_data.to_csv(str(metrics_path), index=False)\n",
    "    test_details.to_csv(str(details_path))\n",
    "    scores.to_csv(str(scores_path), index=False)\n",
    "    sert_data.to_csv(str(data_cache_path), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_details\n",
    "total_score = scores[scores['workload'] == 'All'].dropna(axis=1)\n",
    "total_score = total_score.merge(test_details, left_on='test-name', right_index=True, how='left').dropna()\n",
    "total_score['scenario'] = list(' - '.join(x) for x in zip(total_score['model'], total_score['cpu']))\n",
    "total_score['temperature'] = total_score.apply(lambda row: sert_data[sert_data['test-name'] == row['test-name']]['temp-avg'].mean(), axis=1)\n",
    "total_score['temperature-range'] = total_score.apply(lambda row: sert_data[sert_data['test-name'] == row['test-name']]['temp-max'].max() - sert_data[sert_data['test-name'] == row['test-name']]['temp-min'].min(), axis=1)\n",
    "total_score = total_score.merge(sert_data[sert_data['workload'] == 'Idle'][['watts-avg','test-name']], left_on='test-name', right_on='test-name').rename(columns={'watts-avg':'idle-power'})\n",
    "total_score['max-power'] = total_score.apply(lambda row: sert_data[(sert_data['test-name'] == row['test-name']) & (sert_data['loadlevel']=='100%')]['watts-avg'].mean(), axis=1)\n",
    "total_score['pressure'] = total_score.apply(lambda row: sert_data[sert_data['test-name'] == row['test-name']]['pressure'].mean(), axis=1)\n",
    "total_score['pressure_bound'] = pd.cut(total_score.pressure, [total_score.pressure.min(), 15, total_score.pressure.max()], labels=['low','high'])\n",
    "total_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "def read_coretemp_csv(filepath):\n",
    "    #Read the file\n",
    "    samples = pd.read_csv(filepath, skiprows=8, header=0, index_col=0, parse_dates=['Time'], encoding='cp1252')\n",
    "    if not pd.api.types.is_datetime64_any_dtype(samples.index.dtype):\n",
    "        # Final row contains \"Session end:\"\n",
    "        samples.drop('Session end:', inplace=True)\n",
    "        samples.index = pd.to_datetime(samples.index)\n",
    "\n",
    "    summary = pd.DataFrame()\n",
    "    summary['cpu-power'] = samples['CPU 0 Power'] + samples['CPU 1 Power']\n",
    "    summary['avg-temp'] = samples.filter(regex='Temp').mean(axis=1, skipna=True)\n",
    "    summary['avg-load'] = samples.filter(regex='load').mean(axis=1, skipna=True)\n",
    "    summary['avg-freq'] = samples.filter(regex='speed').mean(axis=1, skipna=True)\n",
    "    summary['source'] = filepath.name\n",
    "    summary.index = summary.index.tz_localize(tz='Europe/London', ambiguous='infer')\n",
    "    return summary\n",
    "\n",
    "def read_ohwm_csv(filepath):\n",
    "    cpu_metrics = pd.read_csv(filepath, header=[0,1], parse_dates=[0], index_col=0)\n",
    "    cpu_metrics.columns = cpu_metrics.columns.get_level_values(0)\n",
    "\n",
    "    #If the column isn't automatically converted then the CSV has been corrupted some how, \n",
    "    # try to recover as much data as possible by dropping any row where the time cannot be \n",
    "    # parsed (this might leave some columns with empty values in the preceeding row)\n",
    "    cpu_metrics['Time'] = cpu_metrics.index\n",
    "    cpu_metrics['Time'] = pd.to_datetime(cpu_metrics['Time'], errors='coerce')\n",
    "    cpu_metrics.dropna(inplace=True)\n",
    "    cpu_metrics.set_index('Time', inplace=True)\n",
    "        \n",
    "    \n",
    "    summary = pd.DataFrame()\n",
    "\n",
    "    summary['avg-temp'] = cpu_metrics.filter(regex='cpu/./temp').mean(axis=1, skipna=True)\n",
    "    summary['avg-load'] = cpu_metrics.filter(regex='cpu/./load').mean(axis=1, skipna=True)\n",
    "    summary['avg-freq'] = cpu_metrics.filter(regex='cpu/./clock').mean(axis=1, skipna=True)\n",
    "    summary['cpu-power'] = cpu_metrics.filter(regex='cpu/./power/0').sum(axis=1) # cpu-package power\n",
    "    summary['source'] = filepath.name\n",
    "    summary.index =  summary.index.tz_localize(tz='Europe/London', ambiguous='infer')\n",
    "    return summary\n",
    "\n",
    "\n",
    "cpu_metrics_file = working_dir.joinpath('cpu_metrics.csv')\n",
    "\n",
    "if not cpu_metrics_file.exists() or overwrite_data:\n",
    "    cpu_summary = pd.DataFrame(columns=['avg-temp', 'avg-load', 'avg-freq', 'cpu-power', 'source'])\n",
    "else:\n",
    "    print('Reading stored metrics...')\n",
    "    cpu_summary = pd.read_csv(cpu_metrics_file, index_col='Time', parse_dates=['Time'])\n",
    "\n",
    "if 'source' not in cpu_summary.columns:\n",
    "    cpu_summary['source'] = ''\n",
    "\n",
    "new_data = False\n",
    "previous_files = cpu_summary['source'].unique()\n",
    "\n",
    "# old format\n",
    "for f in [file for file in Path(cpu_metrics_dir).glob('*.csv') if file.name not in previous_files]:\n",
    "    try:\n",
    "        cpu_summary = cpu_summary.append(read_coretemp_csv(f))\n",
    "        new_data = True\n",
    "    except Exception as e:\n",
    "        print(f'FAILED LOADING FILE: {f} -- {e}')\n",
    "        continue\n",
    "\n",
    "\n",
    "# new format\n",
    "for f in [file for file in Path(cpu_metrics_dir).joinpath('ohwm').glob('*.csv') if file.name not in previous_files]:\n",
    "    try:\n",
    "        cpu_summary = cpu_summary.append(read_ohwm_csv(f))\n",
    "        new_data = True\n",
    "    except Exception as e:\n",
    "        print(f'FAILED LOADING FILE: {f} -- {e}')\n",
    "        continue    \n",
    "\n",
    "#cpu_summary.index = cpu_summary.index.tz_localize(tz='Europe/London', ambiguous='infer')\n",
    "\n",
    "cpu_summary.sort_index(inplace=True)\n",
    "cpu_summary.index.name = 'Time'\n",
    "\n",
    "if new_data:\n",
    "    print('Saving new CSV')\n",
    "    cpu_summary.to_csv(cpu_metrics_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pd.to_datetime(cpu_metrics['Time']\n",
    "if 'cpu-power' not in sert_data.columns:\n",
    "    sert_data = sert_data.reindex(columns= sert_data.columns.tolist() + ['cpu-power', 'cpu-temp', 'cpu-load', 'cpu-freq'])\n",
    "\n",
    "mask = sert_data['cpu-power'].isna()\n",
    "\n",
    "print(f'Filling {mask.sum()} runs')\n",
    "\n",
    "sert_data.loc[mask, 'cpu-power'] = sert_data.loc[mask, :].apply(lambda row: cpu_summary['cpu-power'][row['start']:row['end']].mean(), axis=1)\n",
    "\n",
    "sert_data.loc[mask, 'cpu-temp'] = sert_data.loc[mask, :].apply(lambda row: cpu_summary['avg-temp'][row['start']:row['end']].mean(), axis=1)\n",
    "sert_data.loc[mask, 'cpu-load'] = sert_data.loc[mask, :].apply(lambda row: cpu_summary['avg-load'][row['start']:row['end']].mean(), axis=1)\n",
    "sert_data.loc[mask, 'cpu-freq'] = sert_data.loc[mask, :].apply(lambda row: cpu_summary['avg-freq'][row['start']:row['end']].mean(), axis=1)\n",
    "\n",
    "if mask.any():\n",
    "    sert_data.to_csv(str(data_cache_path), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sert_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sert_data.head()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipped_results = [f.parent.name for f in source_path.glob('**/results.xml') if f.parent.name not in settings.index and not f.parent.joinpath('invalid.png').exists()]\n",
    "print(skipped_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What scenarios have been tested?\n",
    "Using a 3 bin strategy for temperature testing and high and low for pressure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_score.drop(total_score.index[total_score.pressure.isnull()], inplace=True)\n",
    "\n",
    "tested = total_score.groupby(['scenario', 'bios', 'pressure_bound', pd.cut(total_score.temperature,[20, 23.3,26.6, 30])]).size().unstack().unstack()\n",
    "tested.style.applymap(lambda x: 'background-color:pink' if x < 3 else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean\\n', sert_data.groupby('scenario')['temp-avg'].mean())\n",
    "print('\\nVariance\\n', sert_data.groupby('scenario')['temp-avg'].var())\n",
    "sns.displot(data=sert_data, x=\"temp-avg\", hue=\"scenario\", kde=True, fill=False).set(title='Distribution of temperatures for each scenario ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=sert_data, x=\"pressure\", hue=\"scenario\", kde=True, fill=False).set(title='Distribution of differential pressure for each scenario ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effects on overall SERT score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficiency and power consumption measured by SERT\n",
    "\n",
    "For the CPU workelts in particular, we can plot the benchmark load against the efficiency score achevied for each scenario. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu = sert_data[(sert_data['workload'] == 'CPU')| (sert_data['workload'] == 'Idle')]\n",
    "\n",
    "sns.lmplot(x='actual-load', y='efficiency-score', hue='scenario', col='bios', \n",
    "           data=cpu[cpu['temp-avg'] < 23.66], order=2, truncate=True, scatter=True).fig.suptitle('CPU Worklet Efficiency Scores', y=1.1)\n",
    "sns.lmplot(x='actual-load', y='watts-avg', hue='scenario', col='bios', \n",
    "           data=cpu[cpu['temp-avg'] < 23.66], order=2, truncate=True, scatter=True).fig.suptitle('CPU Worklet Power Consumption', y=1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cleaner plot without the individual data plotted for each sert run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='actual-load', y='efficiency-score', hue='scenario', data=cpu[cpu['temp-avg'] < 23.66], order=2, truncate=True, scatter=False)\n",
    "ax = plt.gca()\n",
    "ax.set_title('CPU Worklet Efficiency Scores ( Test Temperature < 23.66C)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environmental conditions for the tests are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Effect of Temperature\n",
    "The overall efficiency score across various temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=total_score, x='temperature', y='efficiency-score', hue='bios', col='pressure_bound', row='scenario')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sert_data.shape , pd.cut(sert_data['temp-avg'],[20, 23.3,26.6, 30]).size)\n",
    "tempdf = sert_data[(sert_data.workload == 'CPU') & (sert_data.pressure < 15) & (sert_data.cpu == 'Intel(R) Xeon(R) CPU E5-2690 0 @ 2.90GHz') & (sert_data.bios == 'Performance')].copy()\n",
    "tempdf['loadlevel'] = tempdf['loadlevel'].apply(lambda x: float(x.strip('%')) / 100)\n",
    "tempdf.groupby(['loadlevel', pd.cut(tempdf['temp-avg'],[20, 23.3,26.6, 30])])['watts-avg'].mean().unstack().pct_change(axis=1).plot()\n",
    "\n",
    "tempdf = sert_data[(sert_data.workload == 'CPU') & (sert_data.pressure < 15) & (sert_data.cpu == 'Intel(R) Xeon(R) CPU E5-2690 0 @ 2.90GHz') & (sert_data.bios == 'Efficiency')].copy()\n",
    "tempdf['loadlevel'] = tempdf['loadlevel'].apply(lambda x: float(x.strip('%')) / 100)\n",
    "tempdf.groupby(['loadlevel', pd.cut(tempdf['temp-avg'],[20, 23.3,26.6, 30])])['watts-avg'].mean().unstack().pct_change(axis=1).plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_score[total_score['temperature'] < 22].groupby(['scenario', 'bios'])[['efficiency-score', 'temperature', 'idle-power', 'max-power']].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_score[total_score['temperature'] > 28].groupby(['scenario', 'bios'])[['efficiency-score', 'temperature', 'idle-power', 'max-power']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu[(cpu['loadlevel'] == '100%')].groupby([ 'worklet','scenario', 'bios'])['norm-score'].mean().unstack().pct_change(axis=1).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu[(cpu.model == 'PowerEdge R640') & (cpu.worklet == 'CryptoAES') & (cpu.loadlevel == '100%')].groupby('bios')['norm-score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu[['worklet', 'loadlevel', 'scenario', 'bios', 'score']].groupby(['scenario', 'bios', 'worklet', 'loadlevel']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPU Power\n",
    "\n",
    "The CPU is usually considered the driver of most power consumption in the server (excluding any expansion cards). During the SERT tests we have also recorded low-level performance registers of the CPU like per-core frequency and also power consumption. \n",
    "\n",
    "Todo: \n",
    "- Determine relationship between chassis and CPU power consumption\n",
    "    - Assume power = P_Idle + P_Chassis + P_CPU\n",
    "    - IS P_Chassis a function of CPU power?\n",
    "    \n",
    "    \n",
    "Read the CPU power data in and summarise for the tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_summary.drop(cpu_summary.index[cpu_summary['cpu-power'] > 1000], inplace=True)\n",
    "#print(cpu_summary.index[cpu_summary['cpu-power'] > 1000])\n",
    "cpu_summary.plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_summary.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking again at the server efficiency scores, but now using the CPU utilsiation dat from the OS rather than the load data calculated by SERT. SERT load is a proportion of the total score/transactions acheived during the calibration runs. OS CPU utilisaiton is the proportion of time the CPU is busy performing operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_worklets = sert_data[((sert_data['workload'] == 'Idle') | (sert_data['workload'] == 'CPU')) & (sert_data['bios'] != 'unknown')]\n",
    "cpu_worklets['scenario'] = list(' - '.join(x) for x in zip(cpu_worklets['model'], cpu_worklets['cpu']))\n",
    "cpu_worklets['pressure-bound'] = pd.cut(cpu_worklets['pressure'], 2, labels=['low', 'high'])\n",
    "sns.lmplot(x='cpu-load', y='efficiency-score', hue='scenario', col='pressure-bound', row='bios', data=cpu_worklets[cpu_worklets['temp-avg'] < 30], order=2, truncate=True, scatter=True).fig.suptitle('Efficiency Scores ( Test Temperature < 30C)', y=1.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 30\n",
    "sns.lmplot(x='cpu-load', y='efficiency-score', col='cpu', hue='bios', data=cpu_worklets[cpu_worklets['temp-avg'] < temperature], order=2, truncate=True, scatter=False).fig.suptitle(f'Efficiency Scores ( Test Temperature < {temperature}C)', y=1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a significantly different relationship than that shown for the SERT load. \n",
    "\n",
    "Breaking down the performance per server and per worklet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='temp-avg', y='efficiency-score', hue='scenario', data=cpu_worklets, order=1, scatter=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPU power consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_worklets['chassis-power'] = cpu_worklets['watts-avg'] - cpu_worklets['cpu-power']\n",
    "\n",
    "plotdf = cpu_worklets.melt('actual-load', ['watts-avg', 'cpu-power', 'chassis-power'])\n",
    "\n",
    "sns.lmplot(x='actual-load', y='value', hue='variable', data=plotdf, order=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for server in cpu_worklets['scenario'].unique():\n",
    "    plotdf = cpu_worklets[cpu_worklets['scenario'] == server].melt(['actual-load', 'bios'], ['watts-avg', 'cpu-power', 'chassis-power'])\n",
    "\n",
    "    sns.lmplot(x='actual-load', y='value', col='bios', hue='variable', data=plotdf, order=2).fig.suptitle(f'Power Breakdown - {server}', y=1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for server in cpu_worklets['scenario'].unique():\n",
    "    plotdf = cpu_worklets[cpu_worklets['scenario'] == server].melt(['cpu-load', 'bios'], ['watts-avg', 'cpu-power', 'chassis-power'])\n",
    "\n",
    "    sns.lmplot(x='cpu-load', y='value', col='bios', hue='variable', data=plotdf, order=2).fig.suptitle(f'Power Breakdown - {server}', y=1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for worklet in sert_data[sert_data['workload'] == 'CPU']['worklet'].unique():\n",
    "    sns.lmplot(data=sert_data[(sert_data['worklet'] == worklet) | (sert_data['workload'] == 'Idle')], x='actual-load', y='cpu-load', hue='scenario', col='bios', order=2).fig.suptitle(f'Server load vs CPU utilisaiton - {worklet}', y=1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu = sert_data[(sert_data['workload'] == 'CPU')| (sert_data['workload'] == 'Idle')]\n",
    "cpu = cpu[cpu['model'] == 'PowerEdge R620']\n",
    "#cpu = cpu[cpu['cpu'].str.contains('E5-2690 0')]\n",
    "\n",
    "\n",
    "#sns.lmplot(x='cpu-power', y='chassis-power', hue='worklet', data=cpu, order=2)\n",
    "sns.scatterplot(x='temp-avg', y='watts-avg', hue='cpu', data=cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='cpu-load', y='cpu-power', data=cpu, order=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.scatter(x=cpu['cpu-temp'], y=cpu['cpu-power'], c=cpu['temp-avg'])\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_ylabel('Ambient Temp', rotation=90)\n",
    "plt.ylabel('CPU Power')\n",
    "plt.xlabel('CPU Temp')\n",
    "plt.title('R620 E5-2690, Efficiency (DPAC) Mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sert_data[(sert_data['test-name'].isin(['sert-0209','sert-0224','sert-0226', 'sert-0227', 'sert-0229','sert-0233','sert-0234','sert-0235'])) & (sert_data['workload'] == 'Idle')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "cdf4359b0957c740623c25fe64b5643f5f27f044169d14d5db8b0ff7aed71d56"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('data-analysis': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
